{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Sentiment Analysis Project Demo\n",
    "\n",
    "This notebook demonstrates the complete Reddit sentiment analysis pipeline. It covers:\n",
    "- Data collection from Reddit\n",
    "- Sentiment analysis of posts and comments\n",
    "- API demonstration\n",
    "- Model evaluation\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "- A Reddit API account and credentials\n",
    "- Python environment set up\n",
    "\n",
    "**Live Demo:** Check out our web application at: https://zmckinney22.github.io/CS410-Group-Project/\n",
    "Github: https://github.com/zmckinney22/CS410-Group-Project\n",
    "\n",
    "Read the README to find out how to get the API credentials for PRAW\n",
    "Enter your Reddit API credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials configured\n"
     ]
    }
   ],
   "source": [
    "# Enter your Reddit API credentials here\n",
    "# Get these from: https://www.reddit.com/prefs/apps\n",
    "REDDIT_CLIENT_ID = \"your_client_id_here\"  # Replace with your actual client ID\n",
    "REDDIT_CLIENT_SECRET = \"your_client_secret_here\"  # Replace with your actual client secret\n",
    "REDDIT_USER_AGENT = \"your_user_agent_here/1.0\"  # e.g., \"RedditSentimentAnalyzer/1.0\"\n",
    "\n",
    "# Validate that credentials are provided\n",
    "if REDDIT_CLIENT_ID == \"your_client_id_here\" or REDDIT_CLIENT_SECRET == \"your_client_secret_here\":\n",
    "    print(\"WARNING: Please replace the placeholder values with your actual Reddit API credentials!\")\n",
    "    print(\"Get them from: https://www.reddit.com/prefs/apps\")\n",
    "else:\n",
    "    print(\"Credentials configured\")\n",
    "\n",
    "# Get current working directory for Jupyter notebook to allow script execution later\n",
    "try:\n",
    "    notebook_dir = globals()['_dh'][0]\n",
    "except:\n",
    "    notebook_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install all required Python packages for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Main dependencies installed\n",
      "API dependencies installed\n",
      "Notebook dependencies installed\n",
      "All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install project dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install all project dependencies\"\"\"\n",
    "    print(\"Installing dependencies...\")\n",
    "    \n",
    "    # Install main requirements\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=notebook_dir\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: Failed to install main dependencies\")\n",
    "            print(\"STDOUT:\", result.stdout)\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "            return False\n",
    "        print(\"Main dependencies installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Exception during main dependencies: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Install API requirements\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"backend/api_requirements.txt\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=notebook_dir\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: Failed to install API dependencies\")\n",
    "            print(\"STDOUT:\", result.stdout)\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "            return False\n",
    "        print(\"API dependencies installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Exception during API dependencies: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Install additional packages needed for the notebook\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"jupyter\", \"matplotlib\", \"pandas\", \"numpy\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=notebook_dir\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: Failed to install notebook dependencies\")\n",
    "            print(\"STDOUT:\", result.stdout)\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "            return False\n",
    "        print(\"Notebook dependencies installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Exception during notebook dependencies: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"All dependencies installed successfully!\")\n",
    "    return True\n",
    "\n",
    "# Run installation\n",
    "install_success = install_dependencies()\n",
    "if not install_success:\n",
    "    print(\"\\nPlease fix dependency installation issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Configure the environment with your Reddit API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables configured\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables for Reddit API\n",
    "import os\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up environment variables for API access\"\"\"\n",
    "    os.environ['REDDIT_CLIENT_ID'] = REDDIT_CLIENT_ID\n",
    "    os.environ['REDDIT_CLIENT_SECRET'] = REDDIT_CLIENT_SECRET\n",
    "    os.environ['REDDIT_USER_AGENT'] = REDDIT_USER_AGENT\n",
    "    \n",
    "    # Verify environment variables are set\n",
    "    required_vars = ['REDDIT_CLIENT_ID', 'REDDIT_CLIENT_SECRET', 'REDDIT_USER_AGENT']\n",
    "    missing = [var for var in required_vars if not os.getenv(var)]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"ERROR: Missing environment variables: {missing}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Environment variables configured\")\n",
    "    return True\n",
    "\n",
    "# Set up environment\n",
    "env_success = setup_environment()\n",
    "if not env_success:\n",
    "    print(\"Please check your Reddit API credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Collection\n",
    "\n",
    "Collect Reddit posts and comments for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reddit data collection...\n",
      "Authenticating with Reddit API...\n",
      "Authentication successful\n",
      "Collecting data from 3 subreddits...\n",
      "- Posts per subreddit: 3\n",
      "- Comments per post: 10\n",
      "Collecting from r/python...\n",
      "  Collected post 'Pandas 3.0 release candidate tagged...' with 10 comments\n",
      "  Collected post 'I built an automated court scraper because finding...' with 10 comments\n",
      "  Collected post 'TIL Python’s random.seed() ignores the sign of int...' with 10 comments\n",
      "Collecting from r/AskReddit...\n",
      "  Collected post 'What's an \"Insider's secret\" from your profession ...' with 10 comments\n",
      "  Collected post 'What is a 'Survival Myth' that people believe beca...' with 10 comments\n",
      "  Collected post 'What do girls “never” tell guys?...' with 10 comments\n",
      "Collecting from r/movies...\n",
      "  Collected post 'First Image from ‘Super Troopers 3’...' with 10 comments\n",
      "  Collected post 'Cary-Hiroyuki Tagawa Dies: ‘Mortal Kombat, ‘Last E...' with 10 comments\n",
      "  Collected post 'The Lack of Class from Quentin Tarantino...' with 10 comments\n",
      "Saved 9 posts to d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\raw_reddit_data.json\n",
      "Collected 9 posts\n",
      "\n",
      "Validating and preprocessing data...\n",
      "\n",
      "Data Validation: Removed 0 invalid posts\n",
      "Kept 9 valid posts with valid comments\n",
      "\n",
      "Duplicate Removal: 9 unique posts, 89 unique comments\n",
      "Saved preprocessed data to data/ folder\n",
      "\n",
      "Generating exploratory data analysis...\n",
      "\n",
      "============================================================\n",
      "EXPLORATORY DATA ANALYSIS REPORT\n",
      "============================================================\n",
      "Total Posts Collected: 9\n",
      "Total Comments Collected: 89\n",
      "Average Comments per Post: 9.89\n",
      "Subreddits Represented: 3\n",
      "\n",
      "Subreddit Distribution:\n",
      "  r/python: 3 posts\n",
      "  r/AskReddit: 3 posts\n",
      "  r/movies: 3 posts\n",
      "\n",
      "Score Statistics:\n",
      "  Average Post Score: 12167.11\n",
      "  Average Comment Score: 3669.26\n",
      "\n",
      "Comment Text Length (characters):\n",
      "  Average: 218.12\n",
      "  Median: 98.00\n",
      "\n",
      "Comment Sentiment (by score):\n",
      "  Positive (score > 0): 87 (97.8%)\n",
      "  Negative (score < 0): 1 (1.1%)\n",
      "\n",
      "Controversy Metrics:\n",
      "  Average Controversy Ratio: 0.009 (0=consensus, 1=max divisive)\n",
      "  Average Comment Score Std Dev: 2810.70 (higher=more mixed opinions)\n",
      "============================================================\n",
      "\n",
      "Data collection and preprocessing complete!\n",
      "\n",
      "Dataset Summary:\n",
      "- Total posts: 9\n",
      "- Total comments: 89\n",
      "- Subreddits: 3\n",
      "- Average comments per post: 9.9\n"
     ]
    }
   ],
   "source": [
    "# Data collection from Reddit\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "def collect_reddit_data():\n",
    "    \"\"\"Collect Reddit data using the project's data collection module\"\"\"\n",
    "    print(\"Starting Reddit data collection...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.reddit import authenticate_reddit, collect_reddit_data, save_raw_data, validate_data_completeness, preprocess_reddit_data, save_preprocessed_data, generate_eda_report\n",
    "        \n",
    "        # Authenticate with Reddit API\n",
    "        print(\"Authenticating with Reddit API...\")\n",
    "        reddit = authenticate_reddit()\n",
    "        print(\"Authentication successful\")\n",
    "        \n",
    "        # Define subreddits to collect from (smaller sample for demo)\n",
    "        subreddits = ['python', 'AskReddit', 'movies']\n",
    "        posts_per_sub = 3  # Reduced for demo\n",
    "        comments_per_post = 10  # Reduced for demo\n",
    "        \n",
    "        print(f\"Collecting data from {len(subreddits)} subreddits...\")\n",
    "        print(f\"- Posts per subreddit: {posts_per_sub}\")\n",
    "        print(f\"- Comments per post: {comments_per_post}\")\n",
    "        \n",
    "        # Collect raw data\n",
    "        raw_data = collect_reddit_data(reddit, subreddits, posts_per_sub, comments_per_post)\n",
    "        save_raw_data(raw_data)\n",
    "        print(f\"Collected {len(raw_data)} posts\")\n",
    "        \n",
    "        # Validate and preprocess data\n",
    "        print(\"\\nValidating and preprocessing data...\")\n",
    "        validated_data = validate_data_completeness(raw_data)\n",
    "        posts_df, comments_df = preprocess_reddit_data(validated_data)\n",
    "        save_preprocessed_data(posts_df, comments_df)\n",
    "        \n",
    "        # Generate EDA report\n",
    "        print(\"\\nGenerating exploratory data analysis...\")\n",
    "        eda_report = generate_eda_report(posts_df, comments_df)\n",
    "        \n",
    "        print(\"Data collection and preprocessing complete!\")\n",
    "        return posts_df, comments_df, eda_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error in data collection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Collect data\n",
    "posts_df, comments_df, eda_report = collect_reddit_data()\n",
    "\n",
    "# Display basic statistics\n",
    "if posts_df is not None and comments_df is not None:\n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"- Total posts: {len(posts_df)}\")\n",
    "    print(f\"- Total comments: {len(comments_df)}\")\n",
    "    print(f\"- Subreddits: {posts_df['subreddit'].nunique()}\")\n",
    "    print(f\"- Average comments per post: {len(comments_df) / len(posts_df):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up SocialSent Lexicons\n",
    "\n",
    "Download and set up SocialSent lexicons for enhanced sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SocialSent lexicons...\n",
      "SocialSent lexicons already installed\n"
     ]
    }
   ],
   "source": [
    "# Set up SocialSent lexicons\n",
    "def setup_socialsent():\n",
    "    \"\"\"Download and set up SocialSent lexicons\"\"\"\n",
    "    print(\"Setting up SocialSent lexicons...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.setup_socialsent import check_installation, setup_socialsent as run_setup\n",
    "        \n",
    "        if check_installation():\n",
    "            print(\"SocialSent lexicons already installed\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Installing SocialSent lexicons (this may take a few minutes)...\")\n",
    "            run_setup()\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR setting up SocialSent: {e}\")\n",
    "        print(\"Continuing without SocialSent (using Liu & Hu lexicon only)\")\n",
    "        return False\n",
    "\n",
    "# Set up SocialSent\n",
    "socialsent_success = setup_socialsent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sentiment Analysis on Collected Comments\n",
    "\n",
    "Analyze the sentiment of collected Reddit comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment of collected comments...\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "Sentiment analyzer initialized (SocialSent: enabled)\n",
      "\n",
      "Analyzing 50 sample comments...\n",
      "\n",
      "Sentiment Distribution:\n",
      "- Negative: 30 (60.0%)\n",
      "- Positive: 17 (34.0%)\n",
      "- Mixed: 2 (4.0%)\n",
      "- Neutral: 1 (2.0%)\n",
      "\n",
      "Sample Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nspewj2</td>\n",
       "      <td>If an animal eats it, you can eat it. Terrible...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>5929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nsri2i8</td>\n",
       "      <td>That we lay an egg once a month</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>18464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ns223gu</td>\n",
       "      <td>In banking, dont be afraid to explicitly ask f...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>4701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nschfbf</td>\n",
       "      <td>Sounds really cool! How difficult will it be t...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nsu1upn</td>\n",
       "      <td>Our favorite bra doesnt get washed as much</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ns3u61o</td>\n",
       "      <td>Congrats to the pandas devs! I appreciate all ...</td>\n",
       "      <td>SentimentLabel.POSITIVE</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nsluoq7</td>\n",
       "      <td>Farva still waiting on his liter of cola...</td>\n",
       "      <td>SentimentLabel.MIXED</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nseeml8</td>\n",
       "      <td>Its scraper... Not scrapper</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nsbyj1z</td>\n",
       "      <td>As someone that has gotten recommended a lawye...</td>\n",
       "      <td>SentimentLabel.POSITIVE</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nt4cry6</td>\n",
       "      <td>Interesting thread, though its important to no...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                               text  \\\n",
       "0    nspewj2  If an animal eats it, you can eat it. Terrible...   \n",
       "1    nsri2i8                    That we lay an egg once a month   \n",
       "2    ns223gu  In banking, dont be afraid to explicitly ask f...   \n",
       "3    nschfbf  Sounds really cool! How difficult will it be t...   \n",
       "4    nsu1upn         Our favorite bra doesnt get washed as much   \n",
       "5    ns3u61o  Congrats to the pandas devs! I appreciate all ...   \n",
       "6    nsluoq7        Farva still waiting on his liter of cola...   \n",
       "7    nseeml8                        Its scraper... Not scrapper   \n",
       "8    nsbyj1z  As someone that has gotten recommended a lawye...   \n",
       "9    nt4cry6  Interesting thread, though its important to no...   \n",
       "\n",
       "                 sentiment  score  \n",
       "0  SentimentLabel.NEGATIVE   5929  \n",
       "1  SentimentLabel.NEGATIVE  18464  \n",
       "2  SentimentLabel.NEGATIVE   4701  \n",
       "3  SentimentLabel.NEGATIVE      7  \n",
       "4  SentimentLabel.NEGATIVE   2810  \n",
       "5  SentimentLabel.POSITIVE    140  \n",
       "6     SentimentLabel.MIXED     96  \n",
       "7  SentimentLabel.NEGATIVE      2  \n",
       "8  SentimentLabel.POSITIVE     47  \n",
       "9  SentimentLabel.NEGATIVE     23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis demonstration\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sample_comments():\n",
    "    \"\"\"Analyze sentiment of a sample of collected comments\"\"\"\n",
    "    print(\"Analyzing sentiment of collected comments...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.sentiment import SentimentAnalyzer\n",
    "        \n",
    "        # Load preprocessed comments\n",
    "        comments_df = pd.read_csv(os.path.join(notebook_dir, 'data/comments_preprocessed.csv'))\n",
    "        posts_df = pd.read_csv(os.path.join(notebook_dir, 'data/posts_preprocessed.csv'))\n",
    "        \n",
    "        # Create sentiment analyzer\n",
    "        analyzer = SentimentAnalyzer(use_socialsent=socialsent_success)\n",
    "        print(f\"Sentiment analyzer initialized (SocialSent: {'enabled' if socialsent_success else 'disabled'})\")\n",
    "        \n",
    "        # Analyze a sample of comments\n",
    "        sample_size = min(50, len(comments_df))  # Analyze up to 50 comments\n",
    "        sample_comments = comments_df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"\\nAnalyzing {sample_size} sample comments...\")\n",
    "        \n",
    "        results = []\n",
    "        for _, comment in sample_comments.iterrows():\n",
    "            sentiment = analyzer.analyze_sentiment(comment['text'])\n",
    "            results.append({\n",
    "                'comment_id': comment['comment_id'],\n",
    "                'text': comment['text'][:100] + '...' if len(comment['text']) > 100 else comment['text'],\n",
    "                'sentiment': sentiment,\n",
    "                'score': comment['score']\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for display\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Show sentiment distribution\n",
    "        sentiment_counts = results_df['sentiment'].value_counts()\n",
    "        print(f\"\\nSentiment Distribution:\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = (count / len(results_df)) * 100\n",
    "            print(f\"- {sentiment.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show sample results\n",
    "        print(f\"\\nSample Results:\")\n",
    "        display(results_df.head(10))\n",
    "        \n",
    "        print(\"Sentiment analysis complete!\")\n",
    "        return results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in sentiment analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run sentiment analysis\n",
    "sentiment_results = analyze_sample_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Example API call Locally\n",
    "This an example call locally in the scenario that Step 5.2 isn't working because the endpoint is down on Render.\n",
    "\n",
    "Start a local server run these commands in your terminal. Make sure you change directory to backend\n",
    "\n",
    "cd backend\n",
    "\n",
    "pip install uvicorn\n",
    "\n",
    "python -m uvicorn main:app --reload --host 127.0.0.1 --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health is <bound method Response.json of <Response [200]>>\n",
      "JSON from https://www.reddit.com/r/UIUC/comments/1p9v9s5/snow_or_cold_does_not_cancel_class/ below:\n",
      " {\n",
      "  \"post_title\": \"Snow or cold DOES NOT cancel class\",\n",
      "  \"overall_sentiment\": \"negative\",\n",
      "  \"groups\": [\n",
      "    {\n",
      "      \"label\": \"positive\",\n",
      "      \"count\": 9,\n",
      "      \"proportion\": 0.1956521739130435\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"negative\",\n",
      "      \"count\": 36,\n",
      "      \"proportion\": 0.782608695652174\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"neutral\",\n",
      "      \"count\": 0,\n",
      "      \"proportion\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"mixed\",\n",
      "      \"count\": 1,\n",
      "      \"proportion\": 0.021739130434782608\n",
      "    }\n",
      "  ],\n",
      "  \"controversy\": 0.15311909262759923,\n",
      "  \"keywords\": [\n",
      "    \"classes\",\n",
      "    \"class\",\n",
      "    \"snow\",\n",
      "    \"canceled\",\n",
      "    \"school\",\n",
      "    \"cancel\",\n",
      "    \"time\",\n",
      "    \"cold\",\n",
      "    \"weather\",\n",
      "    \"professors\"\n",
      "  ],\n",
      "  \"notable_comments\": [\n",
      "    {\n",
      "      \"comment_id\": \"nrew9j9\",\n",
      "      \"snippet\": \"Ha, no.\",\n",
      "      \"sentiment\": \"positive\",\n",
      "      \"score\": 38\n",
      "    },\n",
      "    {\n",
      "      \"comment_id\": \"nrezmoz\",\n",
      "      \"snippet\": \"Me when the massmail canceling classes goes out at 430pm, after everyone's already gone to class\",\n",
      "      \"sentiment\": \"negative\",\n",
      "      \"score\": 247\n",
      "    },\n",
      "    {\n",
      "      \"comment_id\": \"nrf2tvh\",\n",
      "      \"snippet\": \"Absolutely not. This is not a storm. Its just snow.\",\n",
      "      \"sentiment\": \"mixed\",\n",
      "      \"score\": 24\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# put your own Reddit link here or use the example one\n",
    "reddit_link = \"https://www.reddit.com/r/UIUC/comments/1p9v9s5/snow_or_cold_does_not_cancel_class/\"\n",
    "local_health = \"http://127.0.0.1:8000/api/health\"\n",
    "local_endpoint = \"http://127.0.0.1:8000/api/analyze\"\n",
    "\n",
    "# Check health of the API\n",
    "health_request = requests.get(local_health, timeout=10) \n",
    "health_request.raise_for_status() # You'll get an error if the local health fails\n",
    "print(f\"Health is {health_request.json}\")\n",
    "\n",
    "json_url = {\"url\": reddit_link} # the API takes a request called AnalyzeRequest which is just a link\n",
    "r = requests.post(local_endpoint, json=json_url, timeout=(10, 180)) \n",
    "r.raise_for_status() # you'll get an error if the response isn't working\n",
    "\n",
    "print(f\"JSON from {reddit_link} below:\\n {json.dumps(r.json(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Example API call to Render\n",
    "This an example call to Render, the place the API is hosted at, to be able to retrieve information via a Reddit link.\n",
    "\n",
    "**Note:** If the operation times out, try re-running the code cell. Render's free tier can take 50+ seconds to start up on the first request (cold start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health is <bound method Response.json of <Response [200]>>\n",
      "JSON from https://www.reddit.com/r/UIUC/comments/1p9v9s5/snow_or_cold_does_not_cancel_class/ below:\n",
      " {\n",
      "  \"post_title\": \"Snow or cold DOES NOT cancel class\",\n",
      "  \"overall_sentiment\": \"negative\",\n",
      "  \"groups\": [\n",
      "    {\n",
      "      \"label\": \"positive\",\n",
      "      \"count\": 9,\n",
      "      \"proportion\": 0.1956521739130435\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"negative\",\n",
      "      \"count\": 36,\n",
      "      \"proportion\": 0.782608695652174\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"neutral\",\n",
      "      \"count\": 0,\n",
      "      \"proportion\": 0.0\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"mixed\",\n",
      "      \"count\": 1,\n",
      "      \"proportion\": 0.021739130434782608\n",
      "    }\n",
      "  ],\n",
      "  \"controversy\": 0.15311909262759923,\n",
      "  \"keywords\": [\n",
      "    \"classes\",\n",
      "    \"class\",\n",
      "    \"snow\",\n",
      "    \"canceled\",\n",
      "    \"school\",\n",
      "    \"cancel\",\n",
      "    \"time\",\n",
      "    \"cold\",\n",
      "    \"weather\",\n",
      "    \"professors\"\n",
      "  ],\n",
      "  \"notable_comments\": [\n",
      "    {\n",
      "      \"comment_id\": \"nrew9j9\",\n",
      "      \"snippet\": \"Ha, no.\",\n",
      "      \"sentiment\": \"positive\",\n",
      "      \"score\": 39\n",
      "    },\n",
      "    {\n",
      "      \"comment_id\": \"nrezmoz\",\n",
      "      \"snippet\": \"Me when the massmail canceling classes goes out at 430pm, after everyone's already gone to class\",\n",
      "      \"sentiment\": \"negative\",\n",
      "      \"score\": 245\n",
      "    },\n",
      "    {\n",
      "      \"comment_id\": \"nrf2tvh\",\n",
      "      \"snippet\": \"Absolutely not. This is not a storm. Its just snow.\",\n",
      "      \"sentiment\": \"mixed\",\n",
      "      \"score\": 24\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# put your own Reddit link here or use the example one\n",
    "reddit_link = \"https://www.reddit.com/r/UIUC/comments/1p9v9s5/snow_or_cold_does_not_cancel_class/\"\n",
    "# URL used during the project \n",
    "api_endpoint = \"https://cs410-group-project.onrender.com/api/analyze\"\n",
    "health = \" https://cs410-group-project.onrender.com/api/health\"\n",
    "\n",
    "# Check health of the API\n",
    "health_request = requests.get(health, timeout=120) # 120 second timeout, spinup can be 50 secs or more\n",
    "health_request.raise_for_status() # You'll get an error if the API fails\n",
    "print(f\"Health is {health_request.json}\")\n",
    "\n",
    "json_url = {\"url\": reddit_link} # the API takes a request called AnalyzeRequest which is just a link\n",
    "r = requests.post(api_endpoint, json=json_url, timeout=(60, 180)) \n",
    "r.raise_for_status() # you'll get an error if the response isn't working\n",
    "\n",
    "print(f\"JSON from {reddit_link} below:\\n {json.dumps(r.json(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Sentiment Analysis Pipeline (Reddit post URL -> Analysis Results)\n",
    "\n",
    "This demonstrates the complete sentiment analysis pipeline used by our web application.\n",
    "The web app makes API calls to the backend, which uses these same functions.\n",
    "\n",
    "Pipeline: Reddit URL → Fetch post/comments → Analyze sentiment → Return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Reddit post sentiment...\n",
      "Fetching and analyzing: https://www.reddit.com/r/UIUC/comments/1pek4a1/why_are_nighttime_exams_even_allowed/\n",
      "Found 25 comments to analyze\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "\n",
      "ANALYSIS RESULTS\n",
      "==================================================\n",
      "Post Title: Why are nighttime exams even allowed?\n",
      "Overall Sentiment: NEGATIVE\n",
      "Controversy Score: 0.173\n",
      "\n",
      "Sentiment Distribution:\n",
      "  - Positive: 6 comments (24.0%)\n",
      "  - Negative: 18 comments (72.0%)\n",
      "  - Neutral: 1 comments (4.0%)\n",
      "  - Mixed: 0 comments (0.0%)\n",
      "\n",
      "Top Keywords: exams, time, classes, exam, finals, think, work, hours, long, week\n",
      "\n",
      "Notable Comments:\n",
      "  1. [POSITIVE] \"I loved it. Night is when my brain works best.\" (Score: 35)\n",
      "  2. [NEGATIVE] \"Exams usually 2-3 hours long. Hard to find a 2-3 hour time block when everyone is available. My gues...\" (Score: 123)\n",
      "  3. [NEUTRAL] \"asynchronous\" (Score: 7)\n",
      "\n",
      "==================================================\n",
      "Post analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Direct Reddit post analysis demonstration\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "def analyze_reddit_post():\n",
    "    \"\"\"Analyze sentiment of a specific Reddit post directly\"\"\"\n",
    "    print(\"Analyzing Reddit post sentiment...\")\n",
    "    \n",
    "    try:\n",
    "        # Import the backend functions\n",
    "        from backend.reddit import fetch_post_and_comments\n",
    "        from backend.sentiment import analyze_post_and_comments\n",
    "        \n",
    "        # Reddit post URL to analyze\n",
    "        reddit_url = \"https://www.reddit.com/r/UIUC/comments/1pek4a1/why_are_nighttime_exams_even_allowed/\"\n",
    "        print(f\"Fetching and analyzing: {reddit_url}\")\n",
    "        \n",
    "        # Fetch the post and comments\n",
    "        post_data = fetch_post_and_comments(reddit_url, max_comments=50)  # Limit comments for demo\n",
    "        \n",
    "        if not post_data or not post_data.get('comments'):\n",
    "            print(\"ERROR: Could not fetch post data or no comments found\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(post_data['comments'])} comments to analyze\")\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        analysis_result = analyze_post_and_comments(post_data)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nANALYSIS RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Post information\n",
    "        print(f\"Post Title: {analysis_result['post_title']}\")\n",
    "        print(f\"Overall Sentiment: {analysis_result['overall_sentiment'].upper()}\")\n",
    "        print(f\"Controversy Score: {analysis_result['controversy']:.3f}\")\n",
    "        \n",
    "        # Sentiment distribution\n",
    "        print(f\"\\nSentiment Distribution:\")\n",
    "        for group in analysis_result['groups']:\n",
    "            percentage = group['proportion'] * 100\n",
    "            print(f\"  - {group['label'].capitalize()}: {group['count']} comments ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Keywords\n",
    "        if analysis_result['keywords']:\n",
    "            print(f\"\\nTop Keywords: {', '.join(analysis_result['keywords'][:10])}\")\n",
    "        \n",
    "        # Notable comments (show top 3)\n",
    "        if analysis_result['notable_comments']:\n",
    "            print(f\"\\nNotable Comments:\")\n",
    "            for i, comment in enumerate(analysis_result['notable_comments'][:3]):\n",
    "                snippet = comment['snippet'][:100] + \"...\" if len(comment['snippet']) > 100 else comment['snippet']\n",
    "                print(f\"  {i+1}. [{comment['sentiment'].upper()}] \\\"{snippet}\\\" (Score: {comment['score']})\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Post analysis complete\")\n",
    "        \n",
    "        return analysis_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error analyzing post: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Analyze the Reddit post\n",
    "post_analysis = analyze_reddit_post()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Evaluation Datasets\n",
    "\n",
    "Download the SST-2 and Sentiment140 datasets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading evaluation datasets...\n",
      "\n",
      "Downloading SST-2 dataset...\n",
      "\n",
      "======================================================================\n",
      "DOWNLOADING SST-2 DATASET\n",
      "======================================================================\n",
      "\n",
      "dev.tsv already exists, skipping...\n",
      "\n",
      "train.tsv already exists, skipping...\n",
      "\n",
      "SST-2 dataset downloaded successfully!\n",
      "  - dev.tsv: 872 examples\n",
      "  - train.tsv: 6920 examples\n",
      "SST-2 dataset downloaded\n",
      "\n",
      "Downloading Sentiment140 dataset...\n",
      "\n",
      "======================================================================\n",
      "DOWNLOADING SENTIMENT140 DATASET\n",
      "======================================================================\n",
      "WARNING: This is a large file (~80MB, 1.6M tweets)\n",
      "\n",
      "Zip file already exists at d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\sentiment140\\trainingandtestdata.zip\n",
      "\n",
      "Extracting files...\n",
      "Extracted successfully!\n",
      "Found main dataset: d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\sentiment140\\training.1600000.processed.noemoticon.csv\n",
      "Sentiment140 dataset downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download evaluation datasets\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "def download_datasets():\n",
    "    \"\"\"Download evaluation datasets\"\"\"\n",
    "    print(\"Downloading evaluation datasets...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.download_datasets import download_sst2, download_sentiment140\n",
    "        \n",
    "        # Download SST-2 dataset (no confirmation needed)\n",
    "        print(\"\\nDownloading SST-2 dataset...\")\n",
    "        sst2_success = download_sst2()\n",
    "        if sst2_success:\n",
    "            print(\"SST-2 dataset downloaded\")\n",
    "        else:\n",
    "            print(\"Failed to download SST-2 dataset\")\n",
    "            \n",
    "        # Download Sentiment140 dataset with automated confirmation\n",
    "        print(\"\\nDownloading Sentiment140 dataset...\")\n",
    "        sent140_success = download_sentiment140(confirmation_required=False)\n",
    "        if sent140_success:\n",
    "            print(\"Sentiment140 dataset downloaded\")\n",
    "        else:\n",
    "            print(\"Failed to download Sentiment140 dataset\")\n",
    "        \n",
    "        return sst2_success and sent140_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading datasets: {e}\")\n",
    "        return False\n",
    "\n",
    "# Download datasets\n",
    "datasets_success = download_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation\n",
    "\n",
    "Evaluate the sentiment analysis model on benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "\n",
      "EVALUATION RESULTS with SocialSent weight = 0.3\n",
      "Dataset                       Accuracy   Pos/Neg F1       Pos F1       Neg F1       Neu F1     Mixed F1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reddit Manual                   0.6061       0.5820       0.4255       0.7385       0.2105       0.0000\n",
      "SST-2                           0.6778       0.6834       0.6675       0.6993       0.0000       0.0000\n",
      "Sentiment140                    0.6068       0.6188       0.6051       0.6325       0.0000       0.0000\n",
      "\n",
      "Results saved to: d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\evaluation_results.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['python', 'test/test_analyzer_comments.py'],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd=notebook_dir\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the complete Reddit sentiment analysis pipeline:\n",
    "\n",
    "1. **Dependencies Installation**: Installed all required packages\n",
    "2. **Environment Setup**: Configured Reddit API credentials\n",
    "3. **Data Collection**: Collected and preprocessed Reddit data\n",
    "4. **SocialSent Setup**: Downloaded community-specific lexicons\n",
    "5. **Sample Comments Sentiment Analysis**: Analyzed sentiment of sample collected comments\n",
    "    - **5.5 Sample API Call**: Shows how the API call to Render works and looks like\n",
    "6. **Sentiment Analysis given a Reddit URL**: Fetched Reddit post and analyzed sentiment of its comments\n",
    "7. **Download Evaluation Datasets**: Downloaded benchmark datasets\n",
    "8. **Model Evaluation**: Evaluated model performance\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "- **Multi-lexicon sentiment analysis** (Liu & Hu + SocialSent)\n",
    "- **Reddit-specific text preprocessing** (slang, emojis, etc.)\n",
    "- **Context-aware analysis** (negation, intensifiers, subreddit-specific)\n",
    "- **Comprehensive evaluation** on multiple benchmark datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
