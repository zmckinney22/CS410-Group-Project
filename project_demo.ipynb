{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Sentiment Analysis Project Demo\n",
    "\n",
    "This notebook demonstrates the complete Reddit sentiment analysis pipeline. It covers:\n",
    "- Data collection from Reddit\n",
    "- Sentiment analysis of posts and comments\n",
    "- API demonstration\n",
    "- Model evaluation\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "- A Reddit API account and credentials\n",
    "- Python environment set up\n",
    "\n",
    "**Live Demo:** Check out our web application at: https://zmckinney22.github.io/CS410-Group-Project/\n",
    "\n",
    "Enter your Reddit API credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Please replace the placeholder values with your actual Reddit API credentials!\n",
      "Get them from: https://www.reddit.com/prefs/apps\n"
     ]
    }
   ],
   "source": [
    "# Enter your Reddit API credentials here\n",
    "# Get these from: https://www.reddit.com/prefs/apps\n",
    "REDDIT_CLIENT_ID = \"your_client_id_here\"  # Replace with your actual client ID\n",
    "REDDIT_CLIENT_SECRET = \"your_client_secret_here\"  # Replace with your actual client secret\n",
    "REDDIT_USER_AGENT = \"your_user_agent_here/1.0\"  # e.g., \"RedditSentimentAnalyzer/1.0\"\n",
    "\n",
    "# Validate that credentials are provided\n",
    "if REDDIT_CLIENT_ID == \"your_client_id_here\" or REDDIT_CLIENT_SECRET == \"your_client_secret_here\":\n",
    "    print(\"WARNING: Please replace the placeholder values with your actual Reddit API credentials!\")\n",
    "    print(\"Get them from: https://www.reddit.com/prefs/apps\")\n",
    "else:\n",
    "    print(\"Credentials configured\")\n",
    "\n",
    "# Get current working directory for Jupyter notebook to allow script execution later\n",
    "try:\n",
    "    notebook_dir = globals()['_dh'][0]\n",
    "except:\n",
    "    notebook_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install all required Python packages for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Main dependencies installed\n",
      "API dependencies installed\n",
      "Notebook dependencies installed\n",
      "All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install project dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install all project dependencies\"\"\"\n",
    "    print(\"Installing dependencies...\")\n",
    "    \n",
    "    # Install main requirements\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=notebook_dir\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: Failed to install main dependencies\")\n",
    "            print(\"STDOUT:\", result.stdout)\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "            return False\n",
    "        print(\"Main dependencies installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Exception during main dependencies: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Install API requirements\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"backend/api_requirements.txt\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=notebook_dir\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: Failed to install API dependencies\")\n",
    "            print(\"STDOUT:\", result.stdout)\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "            return False\n",
    "        print(\"API dependencies installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Exception during API dependencies: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Install additional packages needed for the notebook\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"jupyter\", \"matplotlib\", \"pandas\", \"numpy\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=notebook_dir\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: Failed to install notebook dependencies\")\n",
    "            print(\"STDOUT:\", result.stdout)\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "            return False\n",
    "        print(\"Notebook dependencies installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Exception during notebook dependencies: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"All dependencies installed successfully!\")\n",
    "    return True\n",
    "\n",
    "# Run installation\n",
    "install_success = install_dependencies()\n",
    "if not install_success:\n",
    "    print(\"\\nPlease fix dependency installation issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Configure the environment with your Reddit API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables configured\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables for Reddit API\n",
    "import os\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up environment variables for API access\"\"\"\n",
    "    os.environ['REDDIT_CLIENT_ID'] = REDDIT_CLIENT_ID\n",
    "    os.environ['REDDIT_CLIENT_SECRET'] = REDDIT_CLIENT_SECRET\n",
    "    os.environ['REDDIT_USER_AGENT'] = REDDIT_USER_AGENT\n",
    "    \n",
    "    # Verify environment variables are set\n",
    "    required_vars = ['REDDIT_CLIENT_ID', 'REDDIT_CLIENT_SECRET', 'REDDIT_USER_AGENT']\n",
    "    missing = [var for var in required_vars if not os.getenv(var)]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"ERROR: Missing environment variables: {missing}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Environment variables configured\")\n",
    "    return True\n",
    "\n",
    "# Set up environment\n",
    "env_success = setup_environment()\n",
    "if not env_success:\n",
    "    print(\"Please check your Reddit API credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Collection\n",
    "\n",
    "Collect Reddit posts and comments for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reddit data collection...\n",
      "Authenticating with Reddit API...\n",
      "Authentication successful\n",
      "Collecting data from 3 subreddits...\n",
      "- Posts per subreddit: 3\n",
      "- Comments per post: 10\n",
      "Collecting from r/python...\n",
      "  Collected post 'Pandas 3.0 release candidate tagged...' with 10 comments\n",
      "  Collected post 'I built an automated court scraper because finding...' with 10 comments\n",
      "  Collected post 'My wife was manually copying YouTube comments, so ...' with 10 comments\n",
      "Collecting from r/AskReddit...\n",
      "  Collected post 'What's an \"Insider's secret\" from your profession ...' with 10 comments\n",
      "  Collected post 'What is a 'Survival Myth' that people believe beca...' with 10 comments\n",
      "  Collected post 'What do girls “never” tell guys?...' with 10 comments\n",
      "Collecting from r/movies...\n",
      "  Collected post 'Hot Fuzz (2007) \"what did he say?\" Dir. Edgar Wrig...' with 10 comments\n",
      "  Collected post 'First Image from ‘Super Troopers 3’...' with 10 comments\n",
      "  Collected post 'Cary-Hiroyuki Tagawa Dies: ‘Mortal Kombat, ‘Last E...' with 10 comments\n",
      "Saved 9 posts to d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\raw_reddit_data.json\n",
      "Collected 9 posts\n",
      "\n",
      "Validating and preprocessing data...\n",
      "\n",
      "Data Validation: Removed 0 invalid posts\n",
      "Kept 9 valid posts with valid comments\n",
      "\n",
      "Duplicate Removal: 9 unique posts, 90 unique comments\n",
      "Saved preprocessed data to data/ folder\n",
      "\n",
      "Generating exploratory data analysis...\n",
      "\n",
      "============================================================\n",
      "EXPLORATORY DATA ANALYSIS REPORT\n",
      "============================================================\n",
      "Total Posts Collected: 9\n",
      "Total Comments Collected: 90\n",
      "Average Comments per Post: 10.00\n",
      "Subreddits Represented: 3\n",
      "\n",
      "Subreddit Distribution:\n",
      "  r/python: 3 posts\n",
      "  r/AskReddit: 3 posts\n",
      "  r/movies: 3 posts\n",
      "\n",
      "Score Statistics:\n",
      "  Average Post Score: 12620.00\n",
      "  Average Comment Score: 3687.93\n",
      "\n",
      "Comment Text Length (characters):\n",
      "  Average: 194.08\n",
      "  Median: 115.50\n",
      "\n",
      "Comment Sentiment (by score):\n",
      "  Positive (score > 0): 89 (98.9%)\n",
      "  Negative (score < 0): 1 (1.1%)\n",
      "\n",
      "Controversy Metrics:\n",
      "  Average Controversy Ratio: 0.010 (0=consensus, 1=max divisive)\n",
      "  Average Comment Score Std Dev: 2443.03 (higher=more mixed opinions)\n",
      "============================================================\n",
      "\n",
      "Data collection and preprocessing complete!\n",
      "\n",
      "Dataset Summary:\n",
      "- Total posts: 9\n",
      "- Total comments: 90\n",
      "- Subreddits: 3\n",
      "- Average comments per post: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Data collection from Reddit\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "def collect_reddit_data():\n",
    "    \"\"\"Collect Reddit data using the project's data collection module\"\"\"\n",
    "    print(\"Starting Reddit data collection...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.reddit import authenticate_reddit, collect_reddit_data, save_raw_data, validate_data_completeness, preprocess_reddit_data, save_preprocessed_data, generate_eda_report\n",
    "        \n",
    "        # Authenticate with Reddit API\n",
    "        print(\"Authenticating with Reddit API...\")\n",
    "        reddit = authenticate_reddit()\n",
    "        print(\"Authentication successful\")\n",
    "        \n",
    "        # Define subreddits to collect from (smaller sample for demo)\n",
    "        subreddits = ['python', 'AskReddit', 'movies']\n",
    "        posts_per_sub = 3  # Reduced for demo\n",
    "        comments_per_post = 10  # Reduced for demo\n",
    "        \n",
    "        print(f\"Collecting data from {len(subreddits)} subreddits...\")\n",
    "        print(f\"- Posts per subreddit: {posts_per_sub}\")\n",
    "        print(f\"- Comments per post: {comments_per_post}\")\n",
    "        \n",
    "        # Collect raw data\n",
    "        raw_data = collect_reddit_data(reddit, subreddits, posts_per_sub, comments_per_post)\n",
    "        save_raw_data(raw_data)\n",
    "        print(f\"Collected {len(raw_data)} posts\")\n",
    "        \n",
    "        # Validate and preprocess data\n",
    "        print(\"\\nValidating and preprocessing data...\")\n",
    "        validated_data = validate_data_completeness(raw_data)\n",
    "        posts_df, comments_df = preprocess_reddit_data(validated_data)\n",
    "        save_preprocessed_data(posts_df, comments_df)\n",
    "        \n",
    "        # Generate EDA report\n",
    "        print(\"\\nGenerating exploratory data analysis...\")\n",
    "        eda_report = generate_eda_report(posts_df, comments_df)\n",
    "        \n",
    "        print(\"Data collection and preprocessing complete!\")\n",
    "        return posts_df, comments_df, eda_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error in data collection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Collect data\n",
    "posts_df, comments_df, eda_report = collect_reddit_data()\n",
    "\n",
    "# Display basic statistics\n",
    "if posts_df is not None and comments_df is not None:\n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"- Total posts: {len(posts_df)}\")\n",
    "    print(f\"- Total comments: {len(comments_df)}\")\n",
    "    print(f\"- Subreddits: {posts_df['subreddit'].nunique()}\")\n",
    "    print(f\"- Average comments per post: {len(comments_df) / len(posts_df):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up SocialSent Lexicons\n",
    "\n",
    "Download and set up SocialSent lexicons for enhanced sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SocialSent lexicons...\n",
      "SocialSent lexicons already installed\n"
     ]
    }
   ],
   "source": [
    "# Set up SocialSent lexicons\n",
    "def setup_socialsent():\n",
    "    \"\"\"Download and set up SocialSent lexicons\"\"\"\n",
    "    print(\"Setting up SocialSent lexicons...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.setup_socialsent import check_installation, setup_socialsent as run_setup\n",
    "        \n",
    "        if check_installation():\n",
    "            print(\"SocialSent lexicons already installed\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Installing SocialSent lexicons (this may take a few minutes)...\")\n",
    "            run_setup()\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR setting up SocialSent: {e}\")\n",
    "        print(\"Continuing without SocialSent (using Liu & Hu lexicon only)\")\n",
    "        return False\n",
    "\n",
    "# Set up SocialSent\n",
    "socialsent_success = setup_socialsent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sentiment Analysis on Collected Comments\n",
    "\n",
    "Analyze the sentiment of collected Reddit comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment of collected comments...\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "Sentiment analyzer initialized (SocialSent: enabled)\n",
      "\n",
      "Analyzing 50 sample comments...\n",
      "\n",
      "Sentiment Distribution:\n",
      "- Negative: 32 (64.0%)\n",
      "- Positive: 14 (28.0%)\n",
      "- Mixed: 3 (6.0%)\n",
      "- Neutral: 1 (2.0%)\n",
      "\n",
      "Sample Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nspgqek</td>\n",
       "      <td>Walking anywhere if you're lost in the wildern...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>6132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ns9fdod</td>\n",
       "      <td>Im interested in the spam list you created, ar...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nsri2i8</td>\n",
       "      <td>That we lay an egg once a month</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>18373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsl37mu</td>\n",
       "      <td>That first film is still fucking hilarious.</td>\n",
       "      <td>SentimentLabel.POSITIVE</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ns3u61o</td>\n",
       "      <td>Congrats to the pandas devs! I appreciate all ...</td>\n",
       "      <td>SentimentLabel.POSITIVE</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nseljtn</td>\n",
       "      <td>Right now, it's keyword-based, so it catches o...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ns2c3z9</td>\n",
       "      <td>So Im a medic. I dont personally care if you t...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>3398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nrwh0dl</td>\n",
       "      <td>The having to get a translator for the transla...</td>\n",
       "      <td>SentimentLabel.MIXED</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nsbyj1z</td>\n",
       "      <td>As someone that has gotten recommended a lawye...</td>\n",
       "      <td>SentimentLabel.POSITIVE</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nspeabj</td>\n",
       "      <td>Pulling out the bullet. They do it in all the ...</td>\n",
       "      <td>SentimentLabel.NEGATIVE</td>\n",
       "      <td>13789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                               text  \\\n",
       "0    nspgqek  Walking anywhere if you're lost in the wildern...   \n",
       "1    ns9fdod  Im interested in the spam list you created, ar...   \n",
       "2    nsri2i8                    That we lay an egg once a month   \n",
       "3    nsl37mu        That first film is still fucking hilarious.   \n",
       "4    ns3u61o  Congrats to the pandas devs! I appreciate all ...   \n",
       "5    nseljtn  Right now, it's keyword-based, so it catches o...   \n",
       "6    ns2c3z9  So Im a medic. I dont personally care if you t...   \n",
       "7    nrwh0dl  The having to get a translator for the transla...   \n",
       "8    nsbyj1z  As someone that has gotten recommended a lawye...   \n",
       "9    nspeabj  Pulling out the bullet. They do it in all the ...   \n",
       "\n",
       "                 sentiment  score  \n",
       "0  SentimentLabel.NEGATIVE   6132  \n",
       "1  SentimentLabel.NEGATIVE      1  \n",
       "2  SentimentLabel.NEGATIVE  18373  \n",
       "3  SentimentLabel.POSITIVE   3281  \n",
       "4  SentimentLabel.POSITIVE    142  \n",
       "5  SentimentLabel.NEGATIVE      1  \n",
       "6  SentimentLabel.NEGATIVE   3398  \n",
       "7     SentimentLabel.MIXED    346  \n",
       "8  SentimentLabel.POSITIVE     48  \n",
       "9  SentimentLabel.NEGATIVE  13789  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis demonstration\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sample_comments():\n",
    "    \"\"\"Analyze sentiment of a sample of collected comments\"\"\"\n",
    "    print(\"Analyzing sentiment of collected comments...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.sentiment import SentimentAnalyzer\n",
    "        \n",
    "        # Load preprocessed comments\n",
    "        comments_df = pd.read_csv(os.path.join(notebook_dir, 'data/comments_preprocessed.csv'))\n",
    "        posts_df = pd.read_csv(os.path.join(notebook_dir, 'data/posts_preprocessed.csv'))\n",
    "        \n",
    "        # Create sentiment analyzer\n",
    "        analyzer = SentimentAnalyzer(use_socialsent=socialsent_success)\n",
    "        print(f\"Sentiment analyzer initialized (SocialSent: {'enabled' if socialsent_success else 'disabled'})\")\n",
    "        \n",
    "        # Analyze a sample of comments\n",
    "        sample_size = min(50, len(comments_df))  # Analyze up to 50 comments\n",
    "        sample_comments = comments_df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"\\nAnalyzing {sample_size} sample comments...\")\n",
    "        \n",
    "        results = []\n",
    "        for _, comment in sample_comments.iterrows():\n",
    "            sentiment = analyzer.analyze_sentiment(comment['text'])\n",
    "            results.append({\n",
    "                'comment_id': comment['comment_id'],\n",
    "                'text': comment['text'][:100] + '...' if len(comment['text']) > 100 else comment['text'],\n",
    "                'sentiment': sentiment,\n",
    "                'score': comment['score']\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for display\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Show sentiment distribution\n",
    "        sentiment_counts = results_df['sentiment'].value_counts()\n",
    "        print(f\"\\nSentiment Distribution:\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = (count / len(results_df)) * 100\n",
    "            print(f\"- {sentiment.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show sample results\n",
    "        print(f\"\\nSample Results:\")\n",
    "        display(results_df.head(10))\n",
    "        \n",
    "        print(\"Sentiment analysis complete!\")\n",
    "        return results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in sentiment analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run sentiment analysis\n",
    "sentiment_results = analyze_sample_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Sentiment Analysis Pipeline (Reddit post URL -> Analysis Results)\n",
    "\n",
    "This demonstrates the complete sentiment analysis pipeline used by our web application.\n",
    "The web app makes API calls to the backend, which uses these same functions.\n",
    "\n",
    "Pipeline: Reddit URL → Fetch post/comments → Analyze sentiment → Return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Reddit post sentiment...\n",
      "Fetching and analyzing: https://www.reddit.com/r/UIUC/comments/1pek4a1/why_are_nighttime_exams_even_allowed/\n",
      "Found 25 comments to analyze\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "\n",
      "ANALYSIS RESULTS\n",
      "==================================================\n",
      "Post Title: Why are nighttime exams even allowed?\n",
      "Overall Sentiment: NEGATIVE\n",
      "Controversy Score: 0.173\n",
      "\n",
      "Sentiment Distribution:\n",
      "  - Positive: 6 comments (24.0%)\n",
      "  - Negative: 18 comments (72.0%)\n",
      "  - Neutral: 1 comments (4.0%)\n",
      "  - Mixed: 0 comments (0.0%)\n",
      "\n",
      "Top Keywords: exams, time, classes, exam, finals, think, work, hours, long, week\n",
      "\n",
      "Notable Comments:\n",
      "  1. [POSITIVE] \"I loved it. Night is when my brain works best.\" (Score: 37)\n",
      "  2. [NEGATIVE] \"Exams usually 2-3 hours long. Hard to find a 2-3 hour time block when everyone is available. My gues...\" (Score: 122)\n",
      "  3. [NEUTRAL] \"asynchronous\" (Score: 7)\n",
      "\n",
      "==================================================\n",
      "Post analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Direct Reddit post analysis demonstration\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "def analyze_reddit_post():\n",
    "    \"\"\"Analyze sentiment of a specific Reddit post directly\"\"\"\n",
    "    print(\"Analyzing Reddit post sentiment...\")\n",
    "    \n",
    "    try:\n",
    "        # Import the backend functions\n",
    "        from backend.reddit import fetch_post_and_comments\n",
    "        from backend.sentiment import analyze_post_and_comments\n",
    "        \n",
    "        # Reddit post URL to analyze\n",
    "        reddit_url = \"https://www.reddit.com/r/UIUC/comments/1pek4a1/why_are_nighttime_exams_even_allowed/\"\n",
    "        print(f\"Fetching and analyzing: {reddit_url}\")\n",
    "        \n",
    "        # Fetch the post and comments\n",
    "        post_data = fetch_post_and_comments(reddit_url, max_comments=50)  # Limit comments for demo\n",
    "        \n",
    "        if not post_data or not post_data.get('comments'):\n",
    "            print(\"ERROR: Could not fetch post data or no comments found\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(post_data['comments'])} comments to analyze\")\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        analysis_result = analyze_post_and_comments(post_data)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nANALYSIS RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Post information\n",
    "        print(f\"Post Title: {analysis_result['post_title']}\")\n",
    "        print(f\"Overall Sentiment: {analysis_result['overall_sentiment'].upper()}\")\n",
    "        print(f\"Controversy Score: {analysis_result['controversy']:.3f}\")\n",
    "        \n",
    "        # Sentiment distribution\n",
    "        print(f\"\\nSentiment Distribution:\")\n",
    "        for group in analysis_result['groups']:\n",
    "            percentage = group['proportion'] * 100\n",
    "            print(f\"  - {group['label'].capitalize()}: {group['count']} comments ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Keywords\n",
    "        if analysis_result['keywords']:\n",
    "            print(f\"\\nTop Keywords: {', '.join(analysis_result['keywords'][:10])}\")\n",
    "        \n",
    "        # Notable comments (show top 3)\n",
    "        if analysis_result['notable_comments']:\n",
    "            print(f\"\\nNotable Comments:\")\n",
    "            for i, comment in enumerate(analysis_result['notable_comments'][:3]):\n",
    "                snippet = comment['snippet'][:100] + \"...\" if len(comment['snippet']) > 100 else comment['snippet']\n",
    "                print(f\"  {i+1}. [{comment['sentiment'].upper()}] \\\"{snippet}\\\" (Score: {comment['score']})\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Post analysis complete\")\n",
    "        \n",
    "        return analysis_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error analyzing post: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Analyze the Reddit post\n",
    "post_analysis = analyze_reddit_post()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Evaluation Datasets\n",
    "\n",
    "Download the SST-2 and Sentiment140 datasets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading evaluation datasets...\n",
      "\n",
      "Downloading SST-2 dataset...\n",
      "\n",
      "======================================================================\n",
      "DOWNLOADING SST-2 DATASET\n",
      "======================================================================\n",
      "\n",
      "Downloading SST-2 dev.tsv...\n",
      "URL: https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/dev.tsv\n",
      "[=====================================================] 100.0%\n",
      "Downloaded to d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\sst2\\dev.tsv\n",
      "\n",
      "Downloading SST-2 train.tsv...\n",
      "URL: https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\n",
      "[==================================================] 100.0%\n",
      "Downloaded to d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\sst2\\train.tsv\n",
      "\n",
      "SST-2 dataset downloaded successfully!\n",
      "  - dev.tsv: 872 examples\n",
      "  - train.tsv: 6920 examples\n",
      "SST-2 dataset downloaded\n",
      "\n",
      "Downloading Sentiment140 dataset...\n",
      "\n",
      "======================================================================\n",
      "DOWNLOADING SENTIMENT140 DATASET\n",
      "======================================================================\n",
      "WARNING: This is a large file (~80MB, 1.6M tweets)\n",
      "\n",
      "Downloading Sentiment140 dataset...\n",
      "URL: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
      "[==================================================] 100.0%\n",
      "Downloaded to d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\sentiment140\\trainingandtestdata.zip\n",
      "\n",
      "Extracting files...\n",
      "Extracted successfully!\n",
      "Found main dataset: d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\sentiment140\\training.1600000.processed.noemoticon.csv\n",
      "Sentiment140 dataset downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download evaluation datasets\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "def download_datasets():\n",
    "    \"\"\"Download evaluation datasets\"\"\"\n",
    "    print(\"Downloading evaluation datasets...\")\n",
    "    \n",
    "    try:\n",
    "        from backend.download_datasets import download_sst2, download_sentiment140\n",
    "        \n",
    "        # Download SST-2 dataset (no confirmation needed)\n",
    "        print(\"\\nDownloading SST-2 dataset...\")\n",
    "        sst2_success = download_sst2()\n",
    "        if sst2_success:\n",
    "            print(\"SST-2 dataset downloaded\")\n",
    "        else:\n",
    "            print(\"Failed to download SST-2 dataset\")\n",
    "            \n",
    "        # Download Sentiment140 dataset with automated confirmation\n",
    "        print(\"\\nDownloading Sentiment140 dataset...\")\n",
    "        sent140_success = download_sentiment140(confirmation_required=False)\n",
    "        if sent140_success:\n",
    "            print(\"Sentiment140 dataset downloaded\")\n",
    "        else:\n",
    "            print(\"Failed to download Sentiment140 dataset\")\n",
    "        \n",
    "        return sst2_success and sent140_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading datasets: {e}\")\n",
    "        return False\n",
    "\n",
    "# Download datasets\n",
    "datasets_success = download_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation\n",
    "\n",
    "Evaluate the sentiment analysis model on benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "Found 3 overlapping words, removing them: ['envious', 'enviously', 'enviousness']\n",
      "Loaded Liu & Hu: 2004 positive, 4780 negative words\n",
      "Loaded SocialSent lexicon 'reddit_general': 9836 words\n",
      "\n",
      "EVALUATION RESULTS with SocialSent weight = 0.3\n",
      "Dataset                       Accuracy   Pos/Neg F1       Pos F1       Neg F1       Neu F1     Mixed F1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reddit Manual                   0.6061       0.5820       0.4255       0.7385       0.2105       0.0000\n",
      "SST-2                           0.6778       0.6834       0.6675       0.6993       0.0000       0.0000\n",
      "Sentiment140                    0.6068       0.6188       0.6051       0.6325       0.0000       0.0000\n",
      "\n",
      "Results saved to: d:\\Shubhi\\COLLEGES\\UIUC\\Assignments\\Fall25\\CS410\\CS410-Group-Project\\data\\evaluation_results.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['python', 'test/test_analyzer_comments.py'],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd=notebook_dir\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the complete Reddit sentiment analysis pipeline:\n",
    "\n",
    "1. **Dependencies Installation**: Installed all required packages\n",
    "2. **Environment Setup**: Configured Reddit API credentials\n",
    "3. **Data Collection**: Collected and preprocessed Reddit data\n",
    "4. **SocialSent Setup**: Downloaded community-specific lexicons\n",
    "5. **Sample Comments Sentiment Analysis**: Analyzed sentiment of sample collected comments\n",
    "6. **Sentiment Analysis given a Reddit URL**: Fetched Reddit post and analyzed sentiment of its comments\n",
    "7. **Download Evaluation Datasets**: Downloaded benchmark datasets\n",
    "8. **Model Evaluation**: Evaluated model performance\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "- **Multi-lexicon sentiment analysis** (Liu & Hu + SocialSent)\n",
    "- **Reddit-specific text preprocessing** (slang, emojis, etc.)\n",
    "- **Context-aware analysis** (negation, intensifiers, subreddit-specific)\n",
    "- **Comprehensive evaluation** on multiple benchmark datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
